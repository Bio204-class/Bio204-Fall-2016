---
title: "Simplist regression estimator"
author: "Paul M. Magwene"
date: "September 21, 2016"
output:
  html_document:
    toc: true
    toc_depth: 2
    theme: readable
    highlight: default  
    fig_width: 5
    fig_height: 3.25
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# A simple algorithm for estimating the slope of a bivariate regression

Let's write a simple function to estimate regression slopes.  This provides a coarse estimation of the slope of the line of best fit. In practice you'd simply use the `lm` (linear model) function or the explicit analytical solution (see hands-on).  Here I am merely trying to give you an intuitive sense of how one might estimate the parameters of a model using a simple algorithm.


```{r}
regr.slope <- function(X,Y, nlines = 250) {
  # mean center data
  ctr.X <- X - mean(X)
  ctr.Y <- Y - mean(Y)
  
  # setup data structures to hold results
  deviations <- rep(NA, nlines)
  slopes <- rep(NA, nlines)
  
  # prefill angles of lines
  angles <- seq(-pi/2 + 0.001, pi/2 - 0.001, length.out = nlines)
  
  ct = 1 # keep track of times through loop
  
  for(angle in angles) {
    slope <- sin(angle)/cos(angle)
    fit.Y <- slope * ctr.X
    deviation.Y <- sum((ctr.Y - fit.Y)^2)
    slopes[ct] = slope
    deviations[ct] = deviation.Y
    ct <- ct + 1
  }
  
  # the best slope is the one for which the squared deviations 
  # are smallest
  best.slope <- slopes[which.min(deviations)]
  
  # return the estimate slope, and all the sloeps and deviations,
  # in the form of a list
  return(list(best.slope = best.slope,
              slopes = slopes, 
              deviations = deviations))
}
```


Having defined this function, let's some up some test data

```{r}
X <- seq(1, 10, length.out = 50) + rnorm(50)
Y <- 1.3 * X + 2 + rnorm(50, sd = 2)  # Y = 3X  + noise
df.XY <- data.frame(X = X, Y = Y)
```

Let's call our function to estimate the regression slope:
```{r}
est.slope <- regr.slope(X, Y)
est.slope$best.slope
```

We can compare that to the analytical solution that `lm` implements:
```{r}
fit.YonX <- lm(Y ~ X, df.XY)
fit.YonX$coefficients[[2]]  # the second coefficient is the slope
```

As we increease the number of lines we evaluate, our estimate should get closer to the analytical solution.
```{r}
est.slope.1000 <- regr.slope(X, Y, nlines = 1000)
est.slope.1000$best.slope
```
Gratifyingly, our expectation of convergence seems to hold!